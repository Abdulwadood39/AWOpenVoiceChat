# TODO

~~Streaming everything (stt, tts, llm), if it'll help latency~~
Streaming stt. llm+tts are streamed at first sentence.

Send llama model to llm instead of instantiating it in the class.
Gives more control over the model. 

Documentation and Doc string for functions

Make it a pip package

Turn Taking model? Retell seems to be doing it. And Vapi ai.

OpenAI GPT API support

Web interface/API. Two websockets, audio in and audio out.

We don't have a good visualizer. And other UI is good.

Tortoise needs to be implemented properly.

Good demo videos showing interruptions and response time etc.

Integrations with popular LLM packages and software

[Silero](https://github.com/snakers4/silero-models) seems to work fast on cpu and has a lot of control over tts.

~~There should be a better way to import different things from stt and tts etc.~~
